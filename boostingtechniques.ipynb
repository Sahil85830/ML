{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da58d207-c710-409a-8b14-8f804a5b13af",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                               #Theoretical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ccb37a-7f00-499a-a463-34ea6ca2a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.  What is Boosting in Machine Learning?\n",
    "'''**Boosting** is an ensemble learning technique in machine learning that combines multiple **weak learners** (typically decision trees) to create a **strong learner** with improved performance.\n",
    "\n",
    "### Key Idea:\n",
    "\n",
    "Boosting trains models sequentially. Each new model focuses on correcting the errors made by the previous ones. It gives more weight to misclassified examples so that subsequent models learn from them.\n",
    "\n",
    "### How it works (in simple steps):\n",
    "\n",
    "1. Start with a weak model.\n",
    "2. Evaluate errors and increase focus (weight) on misclassified points.\n",
    "3. Train the next model to correct those mistakes.\n",
    "4. Repeat and combine the predictions (usually by weighted voting or averaging).\n",
    "\n",
    "### Popular Boosting Algorithms:\n",
    "\n",
    "* **AdaBoost (Adaptive Boosting)**\n",
    "* **Gradient Boosting**\n",
    "* **XGBoost**\n",
    "* **LightGBM**\n",
    "* **CatBoost**\n",
    "\n",
    "### Benefits:\n",
    "\n",
    "* Improves accuracy.\n",
    "* Reduces bias.\n",
    "* Works well with structured/tabular data.\n",
    "\n",
    "### Use cases:\n",
    "\n",
    "* Classification and regression problems.\n",
    "* Widely used in competitions and real-world applications like fraud detection and customer churn prediction.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c253f311-12dd-4c6d-9884-45f12bc9e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.  How does Boosting differ from Bagging?\n",
    "'''**Boosting** and **Bagging** are both ensemble learning methods, but they work in different ways:\n",
    "\n",
    "### Bagging:\n",
    "\n",
    "* Stands for **Bootstrap Aggregating**.\n",
    "* Trains multiple models **independently and in parallel** on different random subsets of the data (with replacement).\n",
    "* All models have equal weight, and their predictions are combined, usually by majority voting or averaging.\n",
    "* It mainly helps to **reduce variance** and prevent overfitting.\n",
    "* A popular example is **Random Forest**.\n",
    "\n",
    "### Boosting:\n",
    "\n",
    "* Trains models **sequentially**, where each new model tries to correct the errors made by the previous ones.\n",
    "* It gives more importance to misclassified points so that future models can learn from them.\n",
    "* The predictions of all models are combined using **weighted voting** or summation.\n",
    "* Boosting helps to **reduce bias and variance**, often achieving higher accuracy.\n",
    "* Examples include **AdaBoost**, **Gradient Boosting**, and **XGBoost**.\n",
    "\n",
    "### In Summary:\n",
    "\n",
    "* Bagging builds models in parallel and focuses on variance reduction.\n",
    "* Boosting builds models sequentially and focuses on learning from mistakes to reduce bias.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e66a7d0-f4e3-4a89-b2ec-fa2379739990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.  What is the key idea behind AdaBoost?\n",
    "'''The **key idea behind AdaBoost (Adaptive Boosting)** is to combine multiple **weak learners** (typically shallow decision trees) in a **sequential** manner to form a **strong classifier**.\n",
    "\n",
    "### Here's how AdaBoost works:\n",
    "\n",
    "1. **Start Simple**: Train a weak learner (e.g., a decision stump) on the original data.\n",
    "2. **Focus on Mistakes**: Increase the weights of the misclassified samples so the next learner focuses more on them.\n",
    "3. **Repeat**: Continue adding learners, each correcting the errors of the previous ones.\n",
    "4. **Combine**: Final prediction is made by a **weighted vote** of all the weak learners, where more accurate models get more say.\n",
    "\n",
    "### Why it works:\n",
    "\n",
    "AdaBoost adapts by giving more importance to examples that are hard to classify, leading to better overall performance.\n",
    "\n",
    "It’s effective in reducing **bias** and improving **accuracy**, especially on datasets where some examples are harder to predict correctly.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3eca03-1904-4d33-9165-1508ea8195a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.  Explain the working of AdaBoost with an example.\n",
    "'''### Working of AdaBoost – Explained with an Example\n",
    "\n",
    "Let’s say we want to classify emails as **spam** or **not spam** using AdaBoost.\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-step Example:\n",
    "\n",
    "#### Step 1: Assign Equal Weights\n",
    "\n",
    "Start with equal weights for all training examples.\n",
    "Suppose we have 5 emails:\n",
    "\n",
    "* 3 are **not spam**\n",
    "* 2 are **spam**\n",
    "  Each gets a weight of 1/5 = 0.2.\n",
    "\n",
    "#### Step 2: Train a Weak Learner\n",
    "\n",
    "Train a weak classifier (like a small decision stump).\n",
    "Say it misclassifies 2 emails.\n",
    "\n",
    "#### Step 3: Calculate Error\n",
    "\n",
    "Calculate the **weighted error** (sum of weights of misclassified samples).\n",
    "Let’s say the error = 0.4\n",
    "\n",
    "#### Step 4: Calculate Classifier Weight\n",
    "\n",
    "Compute how much say this classifier gets in the final decision:\n",
    "\n",
    "$$\n",
    "alpha = (1/2) * log((1 - error) / error)\n",
    "$$\n",
    "\n",
    "#### Step 5: Update Sample Weights\n",
    "\n",
    "* Increase weights for **misclassified** examples (they become more important).\n",
    "* Decrease weights for **correctly classified** examples.\n",
    "\n",
    "#### Step 6: Repeat\n",
    "\n",
    "Train a new weak learner on the **updated weights**.\n",
    "Repeat steps 2–5 for several rounds.\n",
    "\n",
    "---\n",
    "\n",
    "### Final Prediction\n",
    "\n",
    "All weak learners vote on the final class. Their votes are **weighted** by their accuracy (i.e., their α values). The final prediction is the class with the highest total weighted vote.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "AdaBoost improves performance by:\n",
    "\n",
    "* Focusing more on **hard-to-classify** examples.\n",
    "* Combining many weak learners into a **strong ensemble model**.\n",
    "\n",
    "Let me know if you'd like a code example too!\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee54e7f-a89e-4940-914a-8fdc58db6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.  What is Gradient Boosting, and how is it different from AdaBoost?\n",
    "'''### What is Gradient Boosting?\n",
    "\n",
    "**Gradient Boosting** is an ensemble technique where models are trained **sequentially**, and each new model tries to **correct the errors** made by the previous ones by minimizing a **loss function** using **gradient descent**.\n",
    "\n",
    "Instead of adjusting weights like in AdaBoost, Gradient Boosting fits each new model to the **residual errors** (differences between actual and predicted values).\n",
    "\n",
    "---\n",
    "\n",
    "### How it differs from AdaBoost:\n",
    "\n",
    "1. **Error Handling**:\n",
    "\n",
    "   * **AdaBoost** focuses on misclassified examples by adjusting their weights.\n",
    "   * **Gradient Boosting** fits new models to the residuals of previous models using gradients.\n",
    "\n",
    "2. **Loss Function**:\n",
    "\n",
    "   * **AdaBoost** mainly uses exponential loss.\n",
    "   * **Gradient Boosting** can optimize different loss functions (e.g., squared loss, log loss), making it more flexible.\n",
    "\n",
    "3. **Weighting**:\n",
    "\n",
    "   * **AdaBoost** gives weights to training examples.\n",
    "   * **Gradient Boosting** gives weights to **models** based on how well they reduce loss.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "* **Both** build models sequentially and combine them to make strong predictions.\n",
    "* **AdaBoost** adjusts sample weights to focus on hard examples.\n",
    "* **Gradient Boosting** uses gradient descent to reduce prediction errors.\n",
    "\n",
    "Let me know if you'd like a visual or code demo!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a3d208-317b-4367-b6a2-938e29c3fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.  What is the loss function in Gradient Boosting?\n",
    "'''In **Gradient Boosting**, the **loss function** measures how well the model's predictions match the actual values. The boosting process works by minimizing this loss function step-by-step using gradient descent.\n",
    "\n",
    "### Common Loss Functions:\n",
    "\n",
    "#### For Regression:\n",
    "\n",
    "* **Mean Squared Error (MSE)**\n",
    "\n",
    "  $$\n",
    " Loss = (1/n) * Σ (y_i - y_pred_i)^2\n",
    "  $$\n",
    "* **Mean Absolute Error (MAE)**\n",
    "\n",
    "#### For Binary Classification:\n",
    "\n",
    "* **Log Loss (Cross-Entropy Loss)**\n",
    "\n",
    "  $$\n",
    "  Loss = -[y * log(p) + (1 - y) * log(1 - p)]\n",
    "  $$\n",
    "\n",
    "#### For Multiclass Classification:\n",
    "\n",
    "* **Multiclass Log Loss (Softmax + Cross-Entropy)**\n",
    "\n",
    "---\n",
    "\n",
    "### Key Idea:\n",
    "\n",
    "Gradient Boosting doesn't assume a specific loss function—it uses the **gradient (slope) of the loss** to improve the model at each stage. You can customize the loss function based on the problem (regression or classification).\n",
    "\n",
    "So, the loss function in Gradient Boosting depends on the task, and the algorithm uses its gradient to build better models iteratively.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf1aa2a-4416-44ac-863a-26c0a1bef829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.  How does XGBoost improve over traditional Gradient Boosting?\n",
    "'''**XGBoost (Extreme Gradient Boosting)** is an optimized version of traditional Gradient Boosting that offers better performance and efficiency through several enhancements.\n",
    "\n",
    "### Key Improvements of XGBoost over Traditional Gradient Boosting:\n",
    "\n",
    "1. **Regularization**\n",
    "\n",
    "   * Adds **L1 (Lasso)** and **L2 (Ridge)** regularization to the objective function.\n",
    "   * Helps prevent **overfitting**, which traditional Gradient Boosting lacks.\n",
    "\n",
    "2. **Tree Pruning (Post-Pruning)**\n",
    "\n",
    "   * Uses a **depth-first** tree building approach and prunes trees **after** growing them.\n",
    "   * Traditional methods typically stop splitting early (pre-pruning), which may miss optimal structures.\n",
    "\n",
    "3. **Handling Missing Values**\n",
    "\n",
    "   * XGBoost **automatically learns** the best direction to handle missing data during training.\n",
    "   * No need for manual imputation.\n",
    "\n",
    "4. **Parallelization**\n",
    "\n",
    "   * Supports **parallel computation** during training to build trees faster.\n",
    "   * Traditional Gradient Boosting is sequential and slower.\n",
    "\n",
    "5. **Weighted Quantile Sketch**\n",
    "\n",
    "   * Efficient method for finding the best split points on large datasets with high precision.\n",
    "\n",
    "6. **Sparsity Aware**\n",
    "\n",
    "   * Efficient handling of **sparse data** (e.g., data with many zeros or missing values).\n",
    "\n",
    "7. **Cross-validation Built-in**\n",
    "\n",
    "   * XGBoost includes built-in **cross-validation** functionality.\n",
    "\n",
    "8. **Scalability**\n",
    "\n",
    "   * Designed to work efficiently with large datasets and supports distributed computing.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "XGBoost is faster, more regularized, and more robust than traditional Gradient Boosting, making it one of the most popular and effective machine learning algorithms today.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d50d30d-1654-4b80-b403-cf45c9ee42e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. What is the difference between XGBoost and CatBoost?\n",
    "'''Here’s how **XGBoost** and **CatBoost** differ:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Handling Categorical Features:**\n",
    "\n",
    "* **XGBoost:** Requires manual preprocessing of categorical variables (e.g., one-hot encoding).\n",
    "* **CatBoost:** Natively supports categorical features without explicit encoding, using advanced techniques like **ordered target statistics** to handle them efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Algorithm and Implementation:**\n",
    "\n",
    "* **XGBoost:** Uses a traditional gradient boosting approach with second-order gradients and regularization.\n",
    "* **CatBoost:** Uses **ordered boosting** to reduce prediction shift and bias, improving accuracy and stability, especially on small datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Speed and Efficiency:**\n",
    "\n",
    "* **XGBoost:** Fast and efficient with support for parallel and distributed computing.\n",
    "* **CatBoost:** Also fast with GPU support; often faster on datasets with many categorical variables due to native handling.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Dealing with Overfitting:**\n",
    "\n",
    "* **XGBoost:** Uses L1/L2 regularization and tree pruning to prevent overfitting.\n",
    "* **CatBoost:** Incorporates techniques like **ordered boosting** and **random permutations** to reduce overfitting and prediction bias.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Ease of Use:**\n",
    "\n",
    "* **XGBoost:** Powerful but requires more feature engineering for categorical data.\n",
    "* **CatBoost:** Easier to use when working with categorical data, with less preprocessing needed.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "* Use **CatBoost** when you have many categorical features and want an easy, powerful model with less preprocessing.\n",
    "* Use **XGBoost** for general-purpose gradient boosting with fine control over model parameters, especially for numeric-heavy data.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db66e8ab-c72d-4a2f-90a2-af95d0c0eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.  What are some real-world applications of Boosting techniques?\n",
    "'''Boosting techniques are widely used across various real-world applications because of their strong predictive performance. Here are some common areas where boosting shines:\n",
    "\n",
    "1. **Fraud Detection**\n",
    "\n",
    "   * Detecting fraudulent transactions in banking and finance by identifying subtle patterns in transaction data.\n",
    "\n",
    "2. **Customer Churn Prediction**\n",
    "\n",
    "   * Predicting if a customer is likely to stop using a service, helping businesses take preventive actions.\n",
    "\n",
    "3. **Credit Scoring**\n",
    "\n",
    "   * Assessing the creditworthiness of loan applicants to minimize financial risks.\n",
    "\n",
    "4. **Medical Diagnosis**\n",
    "\n",
    "   * Classifying medical images or patient data to detect diseases like cancer, diabetes, or heart conditions.\n",
    "\n",
    "5. **Spam Email Detection**\n",
    "\n",
    "   * Filtering out unwanted or malicious emails from users’ inboxes.\n",
    "\n",
    "6. **Click-Through Rate (CTR) Prediction**\n",
    "\n",
    "   * Predicting how likely users are to click on online ads, important in digital marketing.\n",
    "\n",
    "7. **Recommendation Systems**\n",
    "\n",
    "   * Improving product or content recommendations based on user behavior.\n",
    "\n",
    "8. **Image and Speech Recognition**\n",
    "\n",
    "   * Enhancing the accuracy of identifying objects in images or understanding spoken language.\n",
    "\n",
    "9. **Sentiment Analysis**\n",
    "\n",
    "   * Analyzing text data (like reviews or social media posts) to determine the sentiment or opinion expressed.\n",
    "\n",
    "---\n",
    "\n",
    "Boosting’s ability to combine weak learners and focus on hard examples makes it very effective for these complex prediction tasks.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca35516-bd55-4833-a095-4f77d5f92fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.  How does regularization help in XGBoost?\n",
    "'''Regularization in **XGBoost** helps by **preventing overfitting** and improving the model's generalization to unseen data. Here’s how it works:\n",
    "\n",
    "1. **Types of Regularization in XGBoost:**\n",
    "\n",
    "   * **L1 regularization (Lasso):** Adds a penalty proportional to the absolute value of the leaf weights. This encourages sparsity, effectively pushing some weights to zero and performing feature selection.\n",
    "   * **L2 regularization (Ridge):** Adds a penalty proportional to the square of the leaf weights, discouraging large weights and smoothing the model.\n",
    "\n",
    "2. **Why it helps:**\n",
    "\n",
    "   * By penalizing complex models (with large or many leaf weights), regularization reduces the risk of fitting noise in the training data.\n",
    "   * It encourages simpler, more robust trees that perform better on new data.\n",
    "   * Controls the complexity of each tree, improving stability and reducing variance.\n",
    "\n",
    "3. **In XGBoost’s Objective Function:**\n",
    "\n",
    "   * Regularization terms are added to the loss function, so the training process tries to minimize both the prediction error and the complexity of the model.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**\n",
    "Regularization in XGBoost balances fitting the data well and keeping the model simple, which leads to better performance and less overfitting.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b59f92d-18c0-4e92-a6c0-33075ccbd2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. What are some hyperparameters to tune in Gradient Boosting models?\n",
    "'''Here are some important hyperparameters you can tune in **Gradient Boosting models** to improve performance:\n",
    "\n",
    "1. **n\\_estimators**\n",
    "\n",
    "   * Number of boosting rounds (trees). More trees can improve accuracy but may cause overfitting.\n",
    "\n",
    "2. **learning\\_rate (shrinkage)**\n",
    "\n",
    "   * Step size for each tree’s contribution. Smaller values require more trees but can lead to better generalization.\n",
    "\n",
    "3. **max\\_depth**\n",
    "\n",
    "   * Maximum depth of each tree. Controls model complexity; deeper trees can capture more patterns but may overfit.\n",
    "\n",
    "4. **min\\_samples\\_split**\n",
    "\n",
    "   * Minimum number of samples required to split a node. Helps prevent creating nodes that are too specific.\n",
    "\n",
    "5. **min\\_samples\\_leaf**\n",
    "\n",
    "   * Minimum number of samples required at a leaf node. Prevents leaves with very few samples.\n",
    "\n",
    "6. **subsample**\n",
    "\n",
    "   * Fraction of training samples used for building each tree. Using less than 1.0 adds randomness and helps prevent overfitting.\n",
    "\n",
    "7. **max\\_features**\n",
    "\n",
    "   * Number of features to consider when looking for the best split. Smaller values reduce overfitting and speed up training.\n",
    "\n",
    "8. **loss**\n",
    "\n",
    "   * Loss function to optimize (e.g., deviance for classification, squared error for regression).\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9db42d9-249b-43e9-a2df-c0e3b7bdefc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12. What is the concept of Feature Importance in Boosting?\n",
    "'''**Feature Importance** in boosting models refers to a way of measuring how much each feature contributes to the predictive power of the model.\n",
    "\n",
    "### Key points about Feature Importance in Boosting:\n",
    "\n",
    "* **What it shows:**\n",
    "  It ranks features based on their impact on the model’s predictions.\n",
    "\n",
    "* **How it’s calculated:**\n",
    "  Common methods include:\n",
    "\n",
    "  * **Gain:** Measures the improvement in accuracy (reduction in loss) brought by a feature when it’s used for splitting. Higher gain means more important feature.\n",
    "  * **Frequency (or Weight):** Counts how many times a feature is used to split across all trees. More splits imply higher importance.\n",
    "  * **Cover:** Measures the number of samples affected by splits on the feature, reflecting how broadly it influences the data.\n",
    "\n",
    "* **Why it matters:**\n",
    "\n",
    "  * Helps identify which features the model relies on most.\n",
    "  * Assists in feature selection and dimensionality reduction.\n",
    "  * Improves interpretability and trust in the model.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "Feature Importance in boosting shows the relative contribution of each input variable, helping you understand and optimize your model better.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aed9e1b-b81b-4ae9-be1d-91ca1fa5dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13.  Why is CatBoost efficient for categorical data?\n",
    "'''CatBoost is efficient for categorical data because it **handles categorical features natively** without needing manual preprocessing like one-hot encoding. Here's why:\n",
    "\n",
    "1. **Ordered Target Statistics:**\n",
    "   CatBoost converts categorical features into numerical values using statistics (like average target value) calculated in an **ordered** manner that avoids data leakage. This means it carefully uses only past data to encode current rows during training.\n",
    "\n",
    "2. **No Need for Manual Encoding:**\n",
    "   You don’t have to do one-hot or label encoding beforehand, which saves time and avoids the curse of dimensionality caused by one-hot encoding many categories.\n",
    "\n",
    "3. **Reduces Overfitting:**\n",
    "   By using a special permutation-driven approach to calculate these statistics, CatBoost reduces bias and overfitting compared to naive target encoding.\n",
    "\n",
    "4. **Efficient Handling of High Cardinality:**\n",
    "   Works well even when categorical features have many unique values (high cardinality), which is challenging for other algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "**In short:** CatBoost’s smart and leakage-free encoding method for categorical variables makes it both accurate and efficient on datasets with categorical data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a51029-515e-4d2e-956a-0399849a50b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                    #Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad682d-0e22-43fd-8397-091c6fb157bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14.  Train an AdaBoost Classifier on a sample dataset and print model accuracy.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load sample dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# For simplicity, convert it into a binary classification problem\n",
    "# (e.g., class 0 vs classes 1 and 2)\n",
    "y_binary = (y == 0).astype(int)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize AdaBoost Classifier\n",
    "model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d9a15-3eeb-4df5-94a3-f661ca7ded02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#15.  Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE).\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Load sample regression dataset\n",
    "# Note: sklearn deprecated load_boston; use fetch_california_housing instead\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize AdaBoost Regressor\n",
    "model = AdaBoostRegressor(n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a73e9c-464f-4c36-bbfa-4796437eb1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#16.  Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Load Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train Gradient Boosting Classifier\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6cc1ee-a6d8-452d-a4dd-28ce3fec2b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#17.  Train a Gradient Boosting Regressor and evaluate using R-Squared Score.\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load sample regression dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize Gradient Boosting Regressor\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate R-squared score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared Score: {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c82dc3-ebc4-4eda-a498-1b3325ad3f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.  Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize Gradient Boosting Classifier\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_pred = gb_model.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "\n",
    "# Initialize XGBoost Classifier\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
    "\n",
    "print(f\"Gradient Boosting Accuracy: {gb_accuracy:.3f}\")\n",
    "print(f\"XGBoost Accuracy: {xgb_accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cafeb0-27b4-4102-b77c-c9d7dc50a689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#19.  Train a CatBoost Classifier and evaluate using F1-Score.\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize CatBoost Classifier\n",
    "model = CatBoostClassifier(verbose=0, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate F1-Score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1-Score: {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ac3407-1994-4785-bd61-9033f9343872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#20.  Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE).\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load regression dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize XGBoost Regressor\n",
    "model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0fe9be-acdb-465d-b304-217ce97a1ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#21. Train an AdaBoost Classifier and visualize feature importance.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train AdaBoost Classifier\n",
    "model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(feature_names, importances)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance from AdaBoost Classifier')\n",
    "plt.gca().invert_yaxis()  # Highest importance on top\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b0ad0a-7adf-4b61-b9db-06bda5edd5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#22. Train a Gradient Boosting Regressor and plot learning curves.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize model with a large number of estimators\n",
    "model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Arrays to store training and validation errors\n",
    "train_errors = []\n",
    "val_errors = []\n",
    "\n",
    "# Calculate error for each stage of boosting\n",
    "for y_train_pred, y_val_pred in zip(model.staged_predict(X_train), model.staged_predict(X_val)):\n",
    "    train_errors.append(mean_squared_error(y_train, y_train_pred))\n",
    "    val_errors.append(mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_errors, label='Training MSE')\n",
    "plt.plot(val_errors, label='Validation MSE')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Learning Curves for Gradient Boosting Regressor')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fed4dad-d4b7-4489-887a-bdd4dd2224a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#23. Train an XGBoost Classifier and visualize feature importance.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train XGBoost Classifier\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_importance(model, max_num_features=15, importance_type='weight', show_values=False)\n",
    "plt.title('Feature Importance - XGBoost Classifier')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad14c0-2b61-45cc-95ac-81cf082631a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#24. Train a CatBoost Classifier and plot the confusion matrix.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train CatBoost Classifier\n",
    "model = CatBoostClassifier(verbose=0, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix - CatBoost Classifier')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbf5d79-b90e-46b6-b2ba-3ec15400ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#25. Train an AdaBoost Classifier with different numbers of estimators and compare accuracy.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Different numbers of estimators to try\n",
    "estimators = [10, 50, 100, 200, 300]\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for n in estimators:\n",
    "    model = AdaBoostClassifier(n_estimators=n, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"Estimators: {n} — Accuracy: {acc:.3f}\")\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(estimators, accuracies, marker='o')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('AdaBoost Accuracy vs Number of Estimators')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc798314-77a3-44ea-821a-6947362c77c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#26. Train a Gradient Boosting Classifier and visualize the ROC curve.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Gradient Boosting Classifier\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for the positive class\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, label=f'Gradient Boosting (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0,1], [0,1], 'k--', label='Random Guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Gradient Boosting Classifier')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac959756-3262-487a-baff-94561c089ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#27. Train an XGBoost Regressor and tune the learning rate using GridSearchCV.\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize XGBoost Regressor\n",
    "xgb = XGBRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Define parameter grid for learning rate\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best learning rate:\", grid_search.best_params_['learning_rate'])\n",
    "print(\"Best CV MSE:\", -grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set using best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test MSE with best learning rate: {test_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bb5416-9c30-45be-a2ab-9b47b35c8357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#28. Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting.\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create an imbalanced dataset by downsampling the minority class (class 1)\n",
    "class_0_indices = np.where(y == 0)[0]\n",
    "class_1_indices = np.where(y == 1)[0]\n",
    "\n",
    "# Keep all class 0, reduce class 1 to 20% to simulate imbalance\n",
    "np.random.seed(42)\n",
    "reduced_class_1_indices = np.random.choice(class_1_indices, size=int(0.2 * len(class_1_indices)), replace=False)\n",
    "\n",
    "# Combine indices\n",
    "imbalanced_indices = np.concatenate([class_0_indices, reduced_class_1_indices])\n",
    "\n",
    "X_imbalanced = X[imbalanced_indices]\n",
    "y_imbalanced = y[imbalanced_indices]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imbalanced, y_imbalanced, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train CatBoost without class weights\n",
    "model_no_weights = CatBoostClassifier(verbose=0, random_state=42)\n",
    "model_no_weights.fit(X_train, y_train)\n",
    "y_pred_no_weights = model_no_weights.predict(X_test)\n",
    "\n",
    "# Calculate class weights manually: inverse frequency\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weights_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "# Train CatBoost with class weights\n",
    "model_with_weights = CatBoostClassifier(verbose=0, class_weights=class_weights_dict, random_state=42)\n",
    "model_with_weights.fit(X_train, y_train)\n",
    "y_pred_with_weights = model_with_weights.predict(X_test)\n",
    "\n",
    "# Print results\n",
    "print(\"Without class weights:\")\n",
    "print(classification_report(y_test, y_pred_no_weights))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_no_weights))\n",
    "\n",
    "print(\"\\nWith class weights:\")\n",
    "print(classification_report(y_test, y_pred_with_weights))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_with_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d358e215-8e7e-4030-a69e-76732bc4963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#29. Train an AdaBoost Classifier and analyze the effect of different learning rates.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Different learning rates to try\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.5, 1, 1.5, 2]\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = AdaBoostClassifier(learning_rate=lr, n_estimators=50, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"Learning rate: {lr} — Accuracy: {acc:.3f}\")\n",
    "\n",
    "# Plot accuracy vs learning rate\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(learning_rates, accuracies, marker='o')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Effect of Learning Rate on AdaBoost Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbf23e6-15f4-4bd3-bd2c-bf86a25bf1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#30. Train an XGBoost Classifier for multi-class classification and evaluate using log-loss.\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Load multi-class dataset (Iris)\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train XGBoost Classifier for multi-class\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', objective='multi:softprob', num_class=3, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on test set\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Calculate log loss\n",
    "loss = log_loss(y_test, y_pred_proba)\n",
    "print(f\"Log-Loss on test set: {loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685fe5ea-edf3-4127-b3ea-ca4a753d57f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                               #Theoretical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6565bb1-b9d2-42d6-b9b3-540a42004828",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
    "'''**Logistic Regression** is a statistical method used for predicting binary outcomes, such as yes/no or true/false. Unlike **Linear Regression**,\n",
    "which predicts continuous numerical values, Logistic Regression estimates the probability of a particular class or event occurring. \n",
    "It does this using the **sigmoid function**, which maps predicted values to a range between 0 and 1, making it ideal for classification tasks.\n",
    "While Linear Regression uses a straight-line equation and minimizes **mean squared error**,\n",
    "Logistic Regression uses the **log loss** (or cross-entropy loss) to measure prediction accuracy. \n",
    "Essentially, Linear Regression is suited for prediction problems, whereas Logistic Regression is used for classification.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa894c45-db6f-498d-b2af-f0bb323e1516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.  What is the mathematical equation of Logistic Regression?\n",
    "'''The mathematical equation of **Logistic Regression** is:\n",
    "\n",
    "**P(Y = 1 | X) = 1 / (1 + e^-(β0 + β1·X1 + β2·X2 + ... + βn·Xn))**\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* **P(Y = 1 | X)**: This is the probability that the output Y is 1, given the input features X.\n",
    "* **β0**: The intercept or bias term.\n",
    "* **β1, β2, ..., βn**: The weights or coefficients for the input features X1, X2, ..., Xn.\n",
    "* **e**: Euler's number, approximately equal to 2.718.\n",
    "* The entire formula is called the **sigmoid function**, which converts the result to a value between 0 and 1.\n",
    "\n",
    "This equation helps us estimate the probability that the outcome belongs to class 1.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f418dca9-6e5d-4d28-8934-c0242da2dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.  Why do we use the Sigmoid function in Logistic Regression?\n",
    "'''We use the **sigmoid function** in Logistic Regression because it converts any real-valued number into a value between **0 and 1**, which can be interpreted as a **probability**.\n",
    "\n",
    "### Reasons:\n",
    "\n",
    "1. **Probability Output**: The sigmoid maps the linear combination of inputs (which can be any real number) into the \\[0, 1] range, making it suitable for binary classification.\n",
    "2. **Smooth Gradient**: It provides a smooth curve, which helps in optimizing the model using gradient-based methods.\n",
    "3. **Clear Decision Boundary**: Values close to 0 indicate class 0, and values close to 1 indicate class 1, which makes classification straightforward.\n",
    "\n",
    "In short, the sigmoid function enables Logistic Regression to model **probabilities** and make **binary decisions**.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8411db-695d-4a55-95a5-2ab0e784b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.  What is the cost function of Logistic Regression?\n",
    "'''The **cost function** used in Logistic Regression is called **Log Loss** or **Cross-Entropy Loss**.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "**Cost = -\\[y \\* log(p) + (1 - y) \\* log(1 - p)]**\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* **y** is the actual output (either 0 or 1).\n",
    "* **p** is the predicted probability that the output is 1.\n",
    "* When the prediction is close to the actual value, the cost is low.\n",
    "* When the prediction is far from the actual value, the cost is high.\n",
    "\n",
    "This cost function helps the model learn by giving more penalty to confident but wrong predictions. \n",
    "It is well-suited for binary classification problems.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be073c15-ad25-41e7-bd71-466c5e2dfcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.  What is Regularization in Logistic Regression? Why is it needed?\n",
    "'''**Regularization** in Logistic Regression is a technique used to prevent the model from **overfitting** by adding a penalty term to the cost function. This penalty discourages the model from fitting the noise in the training data, helping it generalize better to new, unseen data.\n",
    "\n",
    "### Why is Regularization needed?\n",
    "\n",
    "* **Overfitting:** Without regularization, the model might become too complex and fit the training data too closely, capturing noise rather than the true pattern.\n",
    "* **Simpler Model:** Regularization helps keep the model simpler by shrinking the coefficients (weights) towards zero.\n",
    "* **Better Generalization:** This improves the model’s performance on new data, making predictions more reliable.\n",
    "\n",
    "There are mainly two types of regularization used in Logistic Regression:\n",
    "\n",
    "* **L1 Regularization (Lasso):** Adds the sum of absolute values of coefficients as penalty. It can shrink some coefficients to zero, effectively performing feature selection.\n",
    "* **L2 Regularization (Ridge):** Adds the sum of squares of coefficients as penalty. It reduces coefficients but usually does not make them exactly zero.\n",
    "\n",
    "In short, regularization helps improve the model’s robustness and avoids overfitting by controlling the size of the coefficients.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa8133a-faeb-4c7f-a497-52538ae32b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.  Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
    "'''Here’s a clear explanation of the differences between **Lasso**, **Ridge**, and **Elastic Net** regression:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Lasso Regression (L1 Regularization)**\n",
    "\n",
    "* Adds a penalty equal to the **absolute value** of the coefficients (sum of |β|).\n",
    "* Can **shrink some coefficients exactly to zero**, which means it can effectively **select important features** and perform feature selection.\n",
    "* Good when you want a sparse model with fewer features.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Ridge Regression (L2 Regularization)**\n",
    "\n",
    "* Adds a penalty equal to the **square of the coefficients** (sum of β²).\n",
    "* Shrinks coefficients towards zero but **does not set any coefficients exactly to zero**.\n",
    "* Helps when many features contribute but you want to reduce their impact to avoid overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Elastic Net Regression**\n",
    "\n",
    "* Combines both L1 (Lasso) and L2 (Ridge) penalties.\n",
    "* Controlled by mixing parameters that balance between the L1 and L2 penalties.\n",
    "* Useful when you want both **feature selection** (like Lasso) and **coefficient shrinkage** (like Ridge).\n",
    "* Works well when there are **many correlated features**.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "* **Lasso** = feature selection + shrinkage\n",
    "* **Ridge** = shrinkage only\n",
    "* **Elastic Net** = balance of both, good for correlated features and flexible control\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf369ff-b87e-491e-81b3-804eeaef5185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.  When should we use Elastic Net instead of Lasso or Ridge?\n",
    "'''We should use **Elastic Net** instead of Lasso or Ridge when:\n",
    "\n",
    "1. **There are many correlated features** in your dataset.\n",
    "\n",
    "   * Lasso tends to pick one feature from a group of correlated ones and ignore the rest, while Elastic Net can keep groups of correlated features together.\n",
    "\n",
    "2. **You want a balance between feature selection and coefficient shrinkage.**\n",
    "\n",
    "   * Elastic Net combines the strengths of both Lasso (which can zero out coefficients) and Ridge (which shrinks coefficients but keeps them), giving more flexibility.\n",
    "\n",
    "3. **Lasso alone is too aggressive or Ridge alone doesn’t reduce enough features.**\n",
    "\n",
    "   * Elastic Net allows tuning how much penalty comes from L1 vs L2, making it more adaptable to different data scenarios.\n",
    "\n",
    "In short, Elastic Net is preferred when your data has **many correlated predictors** and you want both **sparse** (feature selection) and **stable** models.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d611d854-bff8-4a01-bcb1-7417cc697a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.  What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
    "'''The regularization parameter, often denoted as **λ (lambda)**, controls the **strength of regularization** in Logistic Regression.\n",
    "\n",
    "### Impact of λ:\n",
    "\n",
    "* **Large λ (high regularization):**\n",
    "\n",
    "  * Strongly penalizes large coefficients.\n",
    "  * Pushes coefficients closer to zero, making the model simpler and reducing overfitting.\n",
    "  * But if λ is too large, the model can **underfit** by being too simple and missing important patterns.\n",
    "\n",
    "* **Small λ (low regularization):**\n",
    "\n",
    "  * Weak penalty on coefficients.\n",
    "  * Allows the model to fit the training data more closely.\n",
    "  * Can lead to **overfitting**, where the model captures noise instead of the true pattern.\n",
    "\n",
    "### In summary:\n",
    "\n",
    "λ controls the trade-off between **bias** and **variance**—higher λ increases bias but reduces variance, while lower λ reduces bias but increases variance. Choosing the right λ is key to a good model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5835b4-9544-428c-b6e4-e3a2ad7092ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.  What are the key assumptions of Logistic Regression?\n",
    "'''Here are the key assumptions of **Logistic Regression**:\n",
    "\n",
    "1. **Binary Outcome:**\n",
    "   The dependent variable should be binary (two classes), like 0 or 1.\n",
    "\n",
    "2. **Independent Observations:**\n",
    "   The observations should be independent of each other.\n",
    "\n",
    "3. **Linearity of Logit:**\n",
    "   The relationship between the independent variables and the **log-odds** (logit) of the outcome is linear. This means the log of the odds can be expressed as a linear combination of the features.\n",
    "\n",
    "4. **No Multicollinearity:**\n",
    "   Independent variables should not be highly correlated with each other, as multicollinearity can distort the importance of predictors.\n",
    "\n",
    "5. **Large Sample Size:**\n",
    "   Logistic Regression requires a sufficiently large sample size to provide reliable estimates.\n",
    "\n",
    "6. **No Outliers Influencing the Model Excessively:**\n",
    "   Extreme values can affect the model, so data should be checked for outliers.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e626b8d0-f00e-47c7-b3d9-85758a49d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.  What are some alternatives to Logistic Regression for classification tasks?\n",
    "'''Here are some common alternatives to **Logistic Regression** for classification tasks:\n",
    "\n",
    "1. **Decision Trees**\n",
    "\n",
    "   * Models data by splitting it based on feature values, forming a tree of decisions.\n",
    "   * Easy to interpret and can handle non-linear relationships.\n",
    "\n",
    "2. **Random Forest**\n",
    "\n",
    "   * An ensemble of decision trees, improves accuracy by averaging multiple trees to reduce overfitting.\n",
    "\n",
    "3. **Support Vector Machines (SVM)**\n",
    "\n",
    "   * Finds the best boundary (hyperplane) that separates classes with the maximum margin.\n",
    "   * Works well for high-dimensional spaces.\n",
    "\n",
    "4. **K-Nearest Neighbors (KNN)**\n",
    "\n",
    "   * Classifies based on the majority class among the nearest neighbors in the feature space.\n",
    "   * Simple but can be computationally expensive with large data.\n",
    "\n",
    "5. **Naive Bayes**\n",
    "\n",
    "   * Probabilistic classifier based on Bayes’ theorem assuming feature independence.\n",
    "   * Fast and works well with text classification.\n",
    "\n",
    "6. **Neural Networks**\n",
    "\n",
    "   * Models complex patterns using layers of interconnected nodes.\n",
    "   * Powerful but needs more data and computational resources.\n",
    "\n",
    "7. **Gradient Boosting Machines (e.g., XGBoost, LightGBM)**\n",
    "\n",
    "   * Ensemble methods that build models sequentially to correct errors of previous ones.\n",
    "   * Often achieve high accuracy on many classification tasks.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0026de6-15ad-4eb0-a6b8-b5aee6abcb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.  What are Classification Evaluation Metrics?\n",
    "'''**Classification evaluation metrics** are measures used to assess how well a classification model performs. They help understand the model’s accuracy, precision, recall, and overall effectiveness. Here are some key metrics:\n",
    "\n",
    "1. **Accuracy**\n",
    "\n",
    "   * The percentage of correct predictions out of all predictions.\n",
    "   * Useful when classes are balanced.\n",
    "\n",
    "2. **Precision**\n",
    "\n",
    "   * The proportion of true positive predictions out of all positive predictions made by the model.\n",
    "   * Answers: *When the model predicts positive, how often is it correct?*\n",
    "\n",
    "3. **Recall (Sensitivity or True Positive Rate)**\n",
    "\n",
    "   * The proportion of true positives identified out of all actual positives.\n",
    "   * Answers: *Out of all actual positives, how many did the model catch?*\n",
    "\n",
    "4. **F1-Score**\n",
    "\n",
    "   * The harmonic mean of precision and recall.\n",
    "   * Useful when you want a balance between precision and recall.\n",
    "\n",
    "5. **Confusion Matrix**\n",
    "\n",
    "   * A table showing true positives, true negatives, false positives, and false negatives.\n",
    "   * Gives a complete picture of prediction outcomes.\n",
    "\n",
    "6. **ROC Curve & AUC (Area Under Curve)**\n",
    "\n",
    "   * ROC plots true positive rate vs. false positive rate at different thresholds.\n",
    "   * AUC measures the overall ability of the model to discriminate between classes.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1c41d1-d737-4cb6-aac3-e215deb83063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12.  How does class imbalance affect Logistic Regression?\n",
    "'''Class imbalance happens when one class (say, class 0) has many more examples than the other (class 1). This can **negatively affect Logistic Regression** in several ways:\n",
    "\n",
    "1. **Bias Towards Majority Class:**\n",
    "   The model tends to predict the majority class more often because minimizing overall error is easier by doing so, leading to poor detection of the minority class.\n",
    "\n",
    "2. **Poor Probability Estimates:**\n",
    "   Logistic Regression may give biased probability outputs, underestimating the minority class probabilities.\n",
    "\n",
    "3. **Skewed Decision Boundary:**\n",
    "   The threshold that separates classes might not work well since the model is trained mostly on the majority class.\n",
    "\n",
    "4. **Misleading Accuracy:**\n",
    "   Accuracy might look high just because the model correctly predicts the majority class, but it fails on the minority class.\n",
    "\n",
    "---\n",
    "\n",
    "### How to handle class imbalance in Logistic Regression?\n",
    "\n",
    "* Use **resampling techniques** (oversample minority or undersample majority).\n",
    "* Use **class weights** to give more importance to the minority class during training.\n",
    "* Evaluate models with metrics sensitive to imbalance, like **precision, recall, F1-score**, or **AUC** instead of accuracy.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf741e-5743-462f-94e8-6559434ac405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13.  What is Hyperparameter Tuning in Logistic Regression?\n",
    "'''**Hyperparameter tuning** in Logistic Regression means selecting the best values for parameters that control the learning process but aren’t learned from the data directly.\n",
    "\n",
    "### Common hyperparameters in Logistic Regression include:\n",
    "\n",
    "* **Regularization parameter (λ or C):** Controls how much the model is penalized for complexity (smaller λ or larger C means less regularization).\n",
    "* **Type of regularization:** Whether to use L1 (Lasso), L2 (Ridge), or Elastic Net.\n",
    "* **Solver:** The algorithm used to optimize the model, like ‘liblinear’, ‘saga’, etc.\n",
    "\n",
    "### Why tune hyperparameters?\n",
    "\n",
    "Choosing the right hyperparameters helps the model achieve the best balance between **underfitting** and **overfitting**, improving its accuracy and generalization on new data.\n",
    "\n",
    "### How is it done?\n",
    "\n",
    "* Try different combinations of hyperparameters using methods like **grid search** or **random search**.\n",
    "* Evaluate model performance on a validation set or using cross-validation.\n",
    "* Select the hyperparameters that give the best performance.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de59fc93-c2b3-47ec-979e-148b46293948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14.  What are different solvers in Logistic Regression? Which one should be used?\n",
    "'''In Logistic Regression, **solvers** are optimization algorithms used to find the best model parameters by minimizing the cost function. Different solvers have different strengths depending on the dataset size, type of regularization, and computational efficiency.\n",
    "\n",
    "### Common Solvers in Logistic Regression:\n",
    "\n",
    "1. **liblinear**\n",
    "\n",
    "   * Uses a coordinate descent algorithm.\n",
    "   * Works well for small to medium datasets.\n",
    "   * Supports L1 and L2 regularization.\n",
    "   * Good for binary classification.\n",
    "\n",
    "2. **newton-cg**\n",
    "\n",
    "   * Uses a Newton’s method variant.\n",
    "   * Supports only L2 regularization.\n",
    "   * Efficient for large datasets but can be slower.\n",
    "\n",
    "3. **lbfgs**\n",
    "\n",
    "   * Uses an optimization algorithm from the quasi-Newton family.\n",
    "   * Supports only L2 regularization.\n",
    "   * Suitable for large datasets and multiclass problems.\n",
    "   * Often the default solver.\n",
    "\n",
    "4. **sag** (Stochastic Average Gradient)\n",
    "\n",
    "   * An iterative optimization algorithm.\n",
    "   * Efficient for very large datasets.\n",
    "   * Supports L2 regularization.\n",
    "\n",
    "5. **saga**\n",
    "\n",
    "   * An extension of sag.\n",
    "   * Supports both L1 and L2 regularization (and Elastic Net).\n",
    "   * Works well for large datasets and sparse data.\n",
    "\n",
    "---\n",
    "\n",
    "### Which solver to use?\n",
    "\n",
    "* For **small to medium datasets with L1 regularization**, use **liblinear**.\n",
    "* For **large datasets** with L2 regularization, **lbfgs** or **newton-cg** are good choices.\n",
    "* For **very large datasets or sparse data** and if you need L1 or Elastic Net, use **saga**.\n",
    "* If you want a fast and general-purpose solver for most problems, **lbfgs** is a solid default.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad689606-9d1f-4b34-ba29-8b047e2ba67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#15.  How is Logistic Regression extended for multiclass classification?\n",
    "'''Logistic Regression is originally designed for **binary classification**, but it can be extended to handle **multiclass classification** (more than two classes) using these common strategies:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **One-vs-Rest (OvR) / One-vs-All**\n",
    "\n",
    "* Build **one binary classifier per class**.\n",
    "* Each classifier predicts whether the input belongs to its class or not (rest of the classes combined).\n",
    "* For prediction, choose the class whose classifier gives the highest probability.\n",
    "* Simple and widely used.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Multinomial Logistic Regression (Softmax Regression)**\n",
    "\n",
    "* Directly models multiple classes using the **softmax function** instead of sigmoid.\n",
    "* Computes probabilities for each class simultaneously, ensuring they sum to 1.\n",
    "* Learns parameters for all classes together, which can give better performance when classes are mutually exclusive.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "* **One-vs-Rest**: Train multiple binary logistic regressions, easier and works well in many cases.\n",
    "* **Multinomial Logistic Regression**: A single model that predicts all classes at once, more elegant and theoretically consistent.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288bcb8e-d0c9-4fee-8203-161937ccedeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#16.  What are the advantages and disadvantages of Logistic Regression?\n",
    "'''Here’s a quick rundown of the **advantages** and **disadvantages** of Logistic Regression:\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "* **Simple and easy to implement**\n",
    "* **Outputs probabilities**, which gives more information than just class labels\n",
    "* Works well when the relationship between features and log-odds is **linear**\n",
    "* **Efficient and fast** to train, even on large datasets\n",
    "* **Interpretable coefficients**, which help understand feature importance\n",
    "* Can be **regularized** (L1, L2) to prevent overfitting\n",
    "* Performs well for **binary classification** and can be extended to multiclass problems\n",
    "\n",
    "---\n",
    "\n",
    "### Disadvantages:\n",
    "\n",
    "* Assumes a **linear relationship** between features and log-odds, which limits its ability to capture complex patterns\n",
    "* **Not suitable for non-linear problems** without feature engineering or transformations\n",
    "* Can **underperform if classes are not linearly separable**\n",
    "* Sensitive to **outliers** and **multicollinearity** among features\n",
    "* Requires **large sample sizes** for stable and reliable estimates\n",
    "* May struggle with **imbalanced datasets** without proper handling\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6773bf4-d05a-44ac-ac6b-2a13e090d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#17.  What are some use cases of Logistic Regression?\n",
    "'''Here are some common **use cases of Logistic Regression**:\n",
    "\n",
    "1. **Medical Diagnosis:**\n",
    "   Predicting whether a patient has a disease (yes/no) based on symptoms and test results.\n",
    "\n",
    "2. **Spam Detection:**\n",
    "   Classifying emails as spam or not spam.\n",
    "\n",
    "3. **Customer Churn Prediction:**\n",
    "   Predicting whether a customer will leave a service or stay.\n",
    "\n",
    "4. **Credit Scoring:**\n",
    "   Assessing whether a loan applicant is likely to default or repay.\n",
    "\n",
    "5. **Marketing Campaigns:**\n",
    "   Predicting whether a customer will respond to a promotion or offer.\n",
    "\n",
    "6. **Fraud Detection:**\n",
    "   Identifying fraudulent transactions in finance.\n",
    "\n",
    "7. **Click-Through Rate Prediction:**\n",
    "   Estimating the probability that a user clicks on an online ad.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5e70c1-22e1-4364-89e6-a3f39d50e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.  What is the difference between Softmax Regression and Logistic Regression?\n",
    "'''### Logistic Regression:\n",
    "\n",
    "* Used for **binary classification** (two classes).\n",
    "* Uses the **sigmoid function** to model the probability of one class (e.g., class 1) versus the other (class 0).\n",
    "* Outputs a single probability between 0 and 1.\n",
    "\n",
    "---\n",
    "\n",
    "### Softmax Regression (Multinomial Logistic Regression):\n",
    "\n",
    "* Used for **multiclass classification** (more than two classes).\n",
    "* Uses the **softmax function** to model probabilities of all classes simultaneously.\n",
    "* Outputs a probability distribution over all classes (all probabilities sum to 1).\n",
    "\n",
    "---\n",
    "\n",
    "### In summary:\n",
    "\n",
    "* Logistic regression handles **two classes** using sigmoid.\n",
    "* Softmax regression generalizes this to **multiple classes** using softmax.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e7093-b687-4ebc-9996-37c47deaa9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#19.  How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
    "'''Choosing between **One-vs-Rest (OvR)** and **Softmax (Multinomial Logistic Regression)** depends on your dataset and problem specifics:\n",
    "\n",
    "---\n",
    "\n",
    "### When to choose **One-vs-Rest (OvR)**:\n",
    "\n",
    "* When you want a **simpler, more interpretable model** by training separate binary classifiers.\n",
    "* If your classes are **not mutually exclusive** (e.g., multilabel classification).\n",
    "* When your dataset is **small or medium-sized**, since OvR can be easier and faster to train.\n",
    "* If you want flexibility to use different models or hyperparameters for each class.\n",
    "\n",
    "---\n",
    "\n",
    "### When to choose **Softmax Regression**:\n",
    "\n",
    "* When classes are **mutually exclusive** (only one correct class per example).\n",
    "* If you want a **single unified model** that considers all classes simultaneously.\n",
    "* When your dataset is **large enough** to train a more complex model.\n",
    "* When you want potentially **better performance** by capturing relationships between classes.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "* **OvR:** simpler, flexible, good for small datasets and multilabel problems.\n",
    "* **Softmax:** unified, more elegant, better for mutually exclusive multiclass problems with enough data.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d19f00-5ff6-4325-8899-7b328ff8c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#20.  How do we interpret coefficients in Logistic Regression?\n",
    "'''Interpreting coefficients in **Logistic Regression** involves understanding how each feature affects the **log-odds** of the outcome.\n",
    "\n",
    "---\n",
    "\n",
    "### Key points:\n",
    "\n",
    "* Each coefficient (**β**) represents the change in the **log-odds** of the outcome for a one-unit increase in the corresponding feature, keeping other features constant.\n",
    "\n",
    "* If **β is positive**, increasing that feature increases the log-odds (and thus the probability) of the outcome being 1.\n",
    "\n",
    "* If **β is negative**, increasing that feature decreases the log-odds (and probability) of the outcome being 1.\n",
    "\n",
    "---\n",
    "\n",
    "### To make it more intuitive, convert coefficients to **odds ratios**:\n",
    "\n",
    "* **Odds Ratio = exp(β)**\n",
    "* An odds ratio > 1 means the feature **increases** the odds of the outcome.\n",
    "* An odds ratio < 1 means the feature **decreases** the odds of the outcome.\n",
    "\n",
    "---\n",
    "\n",
    "### Example:\n",
    "\n",
    "If a feature’s coefficient β = 0.7, then the odds ratio = exp(0.7) ≈ 2.01.\n",
    "This means a one-unit increase in that feature **doubles the odds** of the outcome being 1.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0ec00c-08dd-4ddf-8bed-18569f70151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                #Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee69077-c197-420b-8f6a-27a930cb9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.  Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# For simplicity, convert to a binary classification problem:\n",
    "# Classify if the species is Setosa (class 0) or not (class 1)\n",
    "y_binary = (y == 0).astype(int)\n",
    "\n",
    "# Split data into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b467b-569b-4947-bc0e-9e3951e2aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.  Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = (iris.target == 0).astype(int)  # Convert to binary classification (Setosa vs. others)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Logistic Regression model with L1 regularization\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear')  # 'liblinear' supports L1\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate and print the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy with L1 Regularization: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f46c87-162e-42d3-8b67-cbd510b84433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.  Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = (iris.target == 0).astype(int)  # Binary classification: Setosa vs others\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Logistic Regression with L2 regularization\n",
    "model = LogisticRegression(penalty='l2', solver='liblinear')  # 'liblinear' works well for small binary problems\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate and print model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy with L2 Regularization: {accuracy:.2f}\")\n",
    "\n",
    "# Print model coefficients\n",
    "print(\"Model Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095498f3-90e5-4372-819f-ecda37a91bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.  Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = (iris.target == 0).astype(int)  # Binary classification: Setosa vs others\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Logistic Regression model with Elastic Net regularization\n",
    "model = LogisticRegression(\n",
    "    penalty='elasticnet',\n",
    "    solver='saga',         # 'saga' is required for elasticnet\n",
    "    l1_ratio=0.5,          # Mix of L1 and L2 (0 = L2 only, 1 = L1 only)\n",
    "    max_iter=10000         # Increase if convergence warning appears\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy with Elastic Net Regularization: {accuracy:.2f}\")\n",
    "\n",
    "# Print model coefficients\n",
    "print(\"Model Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77af57b-61c4-411d-b41d-9be9e82678be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.  Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset (3-class classification problem)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target  # Multiclass labels: 0, 1, 2\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Logistic Regression with One-vs-Rest strategy\n",
    "model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate and print accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Multiclass Model Accuracy (OvR): {accuracy:.2f}\")\n",
    "\n",
    "# Print model coefficients\n",
    "print(\"Model Coefficients:\\n\", model.coef_)\n",
    "print(\"Intercepts:\", model.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c7bf2f-77b8-47e5-8814-44c36c252618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = (iris.target == 0).astype(int)  # Binary classification: Setosa vs others\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = LogisticRegression(solver='liblinear')  # 'liblinear' supports both l1 and l2 penalties\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],        # Regularization strength (lower = stronger regularization)\n",
    "    'penalty': ['l1', 'l2']              # Type of regularization\n",
    "}\n",
    "\n",
    "# Apply GridSearchCV\n",
    "grid = GridSearchCV(model, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "# Print results\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e907431-7cb3-4e6d-960a-8f9fa4f52170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.  Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = (iris.target == 0).astype(int)  # Binary classification: Setosa vs others\n",
    "\n",
    "# Define the model\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Set up Stratified K-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluate model using cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
    "\n",
    "# Print individual and average accuracies\n",
    "print(\"Cross-Validation Accuracies:\", scores)\n",
    "print(\"Average Accuracy:\", np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044e2456-3212-44e7-9c7d-9a5977ed2d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.  Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset from CSV file\n",
    "data = pd.read_csv(\"your_dataset.csv\")  # Replace with your actual CSV file name\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(\"target\", axis=1)  # Replace 'target' with your actual target column name\n",
    "y = data[\"target\"]\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train Logistic Regression model\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af40cabc-0277-4b70-be02-dddfd7d0783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset (Iris dataset for multiclass classification)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define parameter distribution\n",
    "param_dist = {\n",
    "    'C': np.logspace(-4, 4, 20),                   # Range of C values\n",
    "    'penalty': ['l1', 'l2'],                      # L1 and L2 regularization\n",
    "    'solver': ['liblinear', 'saga']               # Solvers that support both penalties\n",
    "}\n",
    "\n",
    "# Initialize Logistic Regression\n",
    "model = LogisticRegression(max_iter=5000, multi_class='ovr')\n",
    "\n",
    "# Apply RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on test set\n",
    "y_pred = random_search.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71c5e93-e565-4f36-bb67-9fa885cee3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.  Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset (3-class classification)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target  # Classes: 0, 1, 2\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Logistic Regression model\n",
    "log_reg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Wrap the model with One-vs-One strategy\n",
    "ovo_model = OneVsOneClassifier(log_reg)\n",
    "\n",
    "# Train the OvO model\n",
    "ovo_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = ovo_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"One-vs-One Logistic Regression Accuracy:\", round(accuracy, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c3fe21-3a4d-4969-b578-4af93a181f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.  Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "\n",
    "# Load a binary classification dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target  # Binary labels: 0 (malignant), 1 (benign)\n",
    "\n",
    "# Split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression(max_iter=10000, solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Visualize confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7605a883-f98f-4c88-ac6f-7cacac2240c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,Recall, and F1-Score.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Load a binary classification dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression(max_iter=10000, solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Precision, Recall, and F1-Score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a385f6-c027-488f-bccd-e25169dc44ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13.  Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2,\n",
    "                           weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression without class weights\n",
    "model_no_weights = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "model_no_weights.fit(X_train, y_train)\n",
    "y_pred_no_weights = model_no_weights.predict(X_test)\n",
    "\n",
    "print(\"Without Class Weights:\")\n",
    "print(classification_report(y_test, y_pred_no_weights))\n",
    "\n",
    "# Train Logistic Regression with balanced class weights\n",
    "model_with_weights = LogisticRegression(class_weight='balanced', max_iter=1000, solver='liblinear')\n",
    "model_with_weights.fit(X_train, y_train)\n",
    "y_pred_with_weights = model_with_weights.predict(X_test)\n",
    "\n",
    "print(\"With Class Weights:\")\n",
    "print(classification_report(y_test, y_pred_with_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af36d71-4a63-4e24-bf8d-38deec32351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14.  Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load Titanic dataset (you can download from: https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv)\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Select features and target\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "target = 'Survived'\n",
    "\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Define preprocessing for numerical columns (impute missing values with median)\n",
    "numeric_features = ['Age', 'SibSp', 'Parch', 'Fare']\n",
    "numeric_transformer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Define preprocessing for categorical columns (impute missing with most frequent, then one-hot encode)\n",
    "categorical_features = ['Pclass', 'Sex', 'Embarked']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessors\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create pipeline with preprocessing and logistic regression\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Split dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\\n\")\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96427cd1-e866-48cc-9a98-0bced1232f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#15.  Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Without Scaling ---\n",
    "model_no_scaling = LogisticRegression(max_iter=10000, solver='liblinear')\n",
    "model_no_scaling.fit(X_train, y_train)\n",
    "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
    "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
    "\n",
    "# --- With Standardization ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model_scaling = LogisticRegression(max_iter=10000, solver='liblinear')\n",
    "model_scaling.fit(X_train_scaled, y_train)\n",
    "y_pred_scaling = model_scaling.predict(X_test_scaled)\n",
    "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
    "\n",
    "# Print comparison\n",
    "print(f\"Accuracy without scaling: {accuracy_no_scaling:.2f}\")\n",
    "print(f\"Accuracy with standardization: {accuracy_scaling:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5e43ed-a095-4093-a3d0-769f74376516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#16.  Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression(max_iter=10000, solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for positive class\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
    "\n",
    "# (Optional) Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0,1], [0,1], 'k--')  # Diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9af7f46-ae4a-404b-a52a-c9b872a0bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#17.  Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression with custom C value\n",
    "model = LogisticRegression(C=0.5, max_iter=10000, solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy with C=0.5: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121da998-ba6b-4dec-b0e9-6dd30190ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.  Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression(max_iter=10000, solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get model coefficients\n",
    "coefficients = model.coef_[0]  # For binary classification, shape (n_features,)\n",
    "\n",
    "# Create a DataFrame to display features and their importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients,\n",
    "    'Absolute Coefficient': np.abs(coefficients)\n",
    "})\n",
    "\n",
    "# Sort features by absolute coefficient value (importance)\n",
    "feature_importance = feature_importance.sort_values(by='Absolute Coefficient', ascending=False)\n",
    "\n",
    "print(\"Feature importance based on coefficients:\")\n",
    "print(feature_importance[['Feature', 'Coefficient']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bbf708-f3b2-495b-8c9c-dca526b6a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression(max_iter=10000, solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Cohen's Kappa Score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "print(f\"Cohen's Kappa Score: {kappa_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b277b2e-636d-4e73-b999-01521f9c09dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#20.  Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression\n",
    "model = LogisticRegression(max_iter=10000, solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for positive class\n",
    "y_scores = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute Precision-Recall curve and average precision score\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "avg_precision = average_precision_score(y_test, y_scores)\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(recall, precision, label=f'Average Precision = {avg_precision:.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb33e539-b8f2-4cff-a6a3-f81de78eb12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#21.  Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# List of solvers to compare\n",
    "solvers = ['liblinear', 'saga', 'lbfgs']\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "for solver in solvers:\n",
    "    # Initialize Logistic Regression with given solver\n",
    "    model = LogisticRegression(max_iter=10000, solver=solver)\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    # Predict on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[solver] = accuracy\n",
    "\n",
    "# Print accuracy for each solver\n",
    "for solver, acc in results.items():\n",
    "    print(f\"Accuracy with solver '{solver}': {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21008bdf-d109-42ae-ad23-288580beac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression(max_iter=10000, solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Matthews Correlation Coefficient\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7e918d-d3b1-43cd-8a05-30a0145c4dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#23.  Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train on raw data\n",
    "model_raw = LogisticRegression(max_iter=10000, solver='liblinear')\n",
    "model_raw.fit(X_train, y_train)\n",
    "y_pred_raw = model_raw.predict(X_test)\n",
    "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train on standardized data\n",
    "model_scaled = LogisticRegression(max_iter=10000, solver='liblinear')\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "print(f\"Accuracy without scaling: {accuracy_raw:.4f}\")\n",
    "print(f\"Accuracy with standardization: {accuracy_scaled:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74e0687-1447-42ad-9c2d-efe7931f8d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#24.  Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Logistic Regression model\n",
    "model = LogisticRegression(max_iter=10000, solver='liblinear')\n",
    "\n",
    "# Define grid of C values to search\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best C value\n",
    "best_C = grid_search.best_params_['C']\n",
    "print(f\"Optimal C: {best_C}\")\n",
    "\n",
    "# Evaluate best model on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy with optimal C: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a926907d-7837-428f-a51b-04a927b90383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#25.  Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
    "\n",
    "import joblib\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression(max_iter=10000, solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model to a file\n",
    "joblib.dump(model, 'logistic_model.joblib')\n",
    "\n",
    "# Load the model from the file\n",
    "loaded_model = joblib.load('logistic_model.joblib')\n",
    "\n",
    "# Make predictions with the loaded model\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of loaded model: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
